layer_idx: &layer_idx 11
name: &name !join [minmo_asr_librispeech+gigaspeech_conformer_chunk240ms-25hz-look_forward-590k-, *layer_idx, "-layers-folded_ffn-no-wd-noise-12.5hz-7b-sft-lora"]
model: extensibletrainer
scale: 1
gpu_ids: [0] # <-- unless you have multiple gpus, use this
start_step: -1 # -1 causes 0.pth to be saved!
checkpointing_enabled: False  # <-- Gradient checkpointing. Enable for huge GPU memory savings. Disable for distributed training.
fp16: &fp16 true # TODO: why does enabling this with 8bit slow down perf??
half_type: &half_type bf16
use_8bit: false
wandb: false  # <-- enable to log to wandb. tensorboard logging is always enabled.
use_tb_logger: true
cuda_benchmarking_enabled: true # no convs in the model
raise_oom: true
ddp_find_unused_parameters: true
anomaly_detection: false
ddp_static_graph: true

exp_dir: &exp_dir
  !join [../experiments/, *name]

qwen2_pretrained_path: &qwen2_pretrained_path
  "qwen/Qwen2.5-7B-Instruct"


text_type: &text_type 0
speech_type: &speech_type 1
asr_text_type: &asr_text_type 2
feature_size: &feature_size 128


manual_seed: &manual_seed 37102
mel_bins: &mel_bins 128

online_stat:
  enabled: false
  stat_batches: &stat_batches 10001
  save_dir: &save_dir /home/wumenglin/cache/peoples_speech/dirty/
  stat_cfg:
    type: MeanVarianceOnlineStats
    feature_dim: 1
    n_dim: *mel_bins
    data_key: &stat_key mel
    data_length_key: mel_lengths

audio_process: &audio_process
  sampling_rate: &sr 16000
  mel: &mel_cfg
    mel_fmin: 0
    mel_fmax: 8000
    sampling_rate: *sr
    n_mel_channels: *mel_bins
    filter_length: 400
    hop_length: 160
    win_length: 400
    true_normalization: false
    mel_norm_file: !join [*save_dir, "/", *stat_key, "_stat_", *stat_batches, ".pt"]


dataloaders:
  train:
    # data loader background
    buffer_background: true
    buffer_background_size: 2
    process_background: true
    process_device: -1
    n_workers: 6
    pin_memory: true

    dataset:
      phase: train
      seed: *manual_seed
      name: LibriSpeech+GigaSpeech
      datasets_list:
        - name: LibriSpeech
          mode: HuggingfaceMinmoASRLhotseDataset
          store_type: json #json
          data_file:
            - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-clean-100.jsonl
            - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-clean-360.jsonl
            - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-other-500.jsonl
          read_keys:
            - text
            # - text_token
            - source
          cache_dir: /home/wumenglin/.cache/cache/jsonl/librispeech_cuts_train/
        - name: GigaSpeech
          mode: HuggingfaceMinmoASRDataset
          store_type: parquet #json
          data_file:
            - /data0/.cache/huggingface/hub/datasets--fixie-ai--gigaspeech/snapshots/dfb1732a2620bb664877f78c1575a85fd201f855/xl-empty-audio-removed/train/*/*.parquet
          read_keys:
            - text
            # - text_token
            - audio
          rm_punc: true
          cache_dir: /home/wumenglin/.cache/cache/parquet/datasets--fixie-ai--gigaspeech/
        - name: peoples_speech_dirty_train
          mode: HuggingfaceMinmoASRDataset
          store_type: parquet #json
          data_file: /home/wumenglin/repo-dev/dl-art-school/codes/scripts/peoples_speech/dirty/train*
          read_keys: 
            - 'audio'
            - 'audio_lengths'
            - text
            # - text_token
          cache_dir: /home/wumenglin/cache/peoples_speech/dirty
        - name: peoples_speech_dirty_sa_train
          mode: HuggingfaceMinmoASRDataset
          store_type: parquet #json
          data_file: /home/wumenglin/repo-dev/dl-art-school/codes/scripts/peoples_speech/dirty_sa/train*
          read_keys: 
            - 'audio'
            - 'audio_lengths'
            - text
            # - text_token
          cache_dir: /home/wumenglin/cache/peoples_speech/dirty_sa
        - name: peoples_speech_clean_train
          mode: HuggingfaceMinmoASRDataset
          store_type: parquet #json
          data_file: /home/wumenglin/repo-dev/dl-art-school/codes/scripts/peoples_speech/clean/train*
          read_keys: 
            - 'audio'
            - 'audio_lengths'
            - text
            # - text_token
          cache_dir: /home/wumenglin/cache/peoples_speech/clean
        - name: peoples_speech_clean_sa_train
          mode: HuggingfaceMinmoASRDataset
          store_type: parquet #json
          data_file: /home/wumenglin/repo-dev/dl-art-school/codes/scripts/peoples_speech/clean_sa/train*
          read_keys: 
            - 'audio'
            - 'audio_lengths'
            - text
            # - text_token
          cache_dir: /home/wumenglin/cache/peoples_speech/clean_sa
      min_duration: 0.3
      max_duration: 30
      sample_duration: 40
      file_path_key: id
      cache_text: false
      token_frame_rate2: 12.5
      shrink: false
      sorted: false
      carry_filename: false

      tokenizers:
        text: *qwen2_pretrained_path
      text_hint_id: 6562
      audio_eos_id: 6563
      text_inter_len: 5
      audio_inter_len: 15
      ignore_id: -100
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type

    sampler:
      batch_size: 4
      buffer_batch_group: 2
      bucket_batch_volume: 32
      similar_type: speech_token_length
      last_samples: 'drop'
      shuffle: true
      limited_length_type: speech_token_length
      length_range: [0, -1]
      copies: 1
      persistent_workers: True
      pad_to_samples: 200
      batch_mode: dynamical
      num_buckets: 16
      ## dynamical data sampler
      max_tokens: 24_000_000_000 # 15 * 768 ^ 1.2 # 34802, 43503 120000/a100
      acc_coeffs: [[2.0, 480], [1.0, 1400000]]
      max_samples: 256
      # partition_record_path: /home/wumenglin/.cache/LibriSpeech+GigaSpeech/dynamic_partition_g50_1d-speech-no-shuffle-noise

      ## bucketed data sampler
      bucket_max_batch_tokens: 16384
      bucket_min_samples: 1
      bucket_max_samples: 180

    collator:
      mode: MelsInferCollator
      apply_half: *fp16
      half_type: *half_type

      spec_fn: 'canonical'
      audio_process: *audio_process
      pad_mode: trim
      data_cfg:
        input_ids:
          padding_val: 151643
        output_ids:
          padding_val: -100
        input_type:
          padding_val: 3
        input_lengths:
          padding_val: -1
        audio:
          padding_val: 0
        audio_lengths:
          padding_val: -1

  val:
    buffer_background: false
    buffer_background_size: 2
    process_background: true
    process_device: 0
    n_workers: 8
    pin_memory: true

    dataset:
      phase: val
      name: LibriSpeech
      seed: *manual_seed
      mode: HuggingfaceMinmoASRLhotseDataset
      store_type: json #json
      data_file:
      - /home/wumenglin/data/librispeech-asr/librispeech_cuts_dev-clean.jsonl
      - /home/wumenglin/data/librispeech-asr/librispeech_cuts_dev-other.jsonl
      cache_dir: /home/wumenglin/.cache/
      partition_record_path: /home/wumenglin/.cache/LibriSpeech/dynamic_partition_6
      min_duration: 0.3
      max_duration: 30
      sample_duration: 40
      file_path_key: id
      cache_text: false
      token_frame_rate2: 12.5
      read_keys:
        - text
        # - text_token
        - source
      shrink: false
      sorted: false
      carry_filename: false

      tokenizers:
        text: *qwen2_pretrained_path
      text_hint_id: 6562
      audio_eos_id: 6563
      text_inter_len: 5
      audio_inter_len: 15
      ignore_id: -100
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type

    sampler:
      batch_size: 24
      buffer_batch_group: 2
      bucket_batch_volume: 32
      similar_type: speech_token_length
      last_samples: 'drop'
      shuffle: true
      limited_length_type: speech_token_length
      length_range: [0, -1]
      copies: 1
      persistent_workers: True
      pad_to_samples: 200
      batch_mode: fixed
      num_buckets: 16
      ## dynamical data sampler
      max_tokens: 8000000000 # 15 * 768 ^ 1.2 # 34802, 43503 120000/a100
      acc_coeffs: [[2.0, 480], [1.0, 1400000]]
      max_samples: 256

      ## bucketed data sampler
      bucket_max_batch_tokens: 16384
      bucket_min_samples: 1
      bucket_max_samples: 180

    collator:
      mode: MelsInferCollator
      apply_half: *fp16
      half_type: *half_type

      spec_fn: 'canonical'
      audio_process: *audio_process
      pad_mode: trim
      data_cfg:
        input_ids:
          padding_val: 151643
        output_ids:
          padding_val: -100
        input_type:
          padding_val: 3
        input_lengths:
          padding_val: -1
        audio:
          padding_val: 0
        audio_lengths:
          padding_val: -1

num_model_dim: &num_model_dim 1024
networks:
  gpt:
    type: generator 
    which_model_G: qwen2_asr # none of the unified_voice*.py files actually match the tortoise inference code... 4 and 3 have "alignment_head" (wtf is that?), 2 lacks the types=1 parameter.
    save_freeze: false
    kwargs:
      pretrain_path: *qwen2_pretrained_path
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type
      feature_size: *feature_size
      projector_type: transformer
      stream: true
      speech_encoder_type: conformer
      encoder:
        class_name: model.audio.module.bestrq_flex.BestRqConformerEncoder
        input_dim: *mel_bins
        input_channels: 1
        num_attention_heads: 8
        hidden_size: *num_model_dim
        ffn_dim: 4096
        num_hidden_layers: 24
        conv_depthwise_kernel_size: 4
        feat_proj_dropout: 0.
        activation_dropout: 0.
        hidden_dropout: 0.
        max_source_positions: 3000
        no_scale_embedding: false
        hidden_act: "swish"
        conformer_conv_dropout: 0.
        position_embeddings_type: relative
        attention_dropout: 0.
        rotary_embedding_base: 10000
        layerdrop: 0.
        final_dropout: 0.
        num_preconformer_layers: 0
        num_preconformer_heads: 4
        preconformer_hidden_size: 384
        preconformer_ffn_dim: 1536
        preconformer_input_feature_projection: false
        causal: true
        sub_layers: 2
        conv_hidden_size: [8, 32]
        compile: false
        window_size: 256
        stream_chunk_size: null #160ms
        lookforward_size: 6 # 240ms  240 / 10 / 2 = 12
        final_LN: false
    
      encoder_pretrained: /home/wumenglin/repo-dev/DL-Art-School-dev/experiments/streamconformer-bestrq_gelu-causal-sub1-stat-ft240ms-v2-no-final-ln-fix-mask-25hz/models/1010000_generator.pth
      encoder_freeze: false
      encoder_layer_idx: *layer_idx
      encoder_freeze_end_layer_idx: -1
      enable_output_ln: false
      use_lora: true
      lora_alpha: 32
      lora_dropout: 0.1
      lora_r: 8
      lora_bias: "none"

steps:
  gpt_train:
    training: gpt
    loss_log_buffer: 500 # no idea what this does
    check_abnorm_grads: true
    # Generally follows the recipe from the DALLE paper.
    optimizer: adamw # this should be adamw_zero if you're using distributed training
    #optimizer: lion
    optimizer_params:
      lr: !!float 1e-4 # CHANGEME: this was originally 1e-4; I reduced it to 1e-5 because it's fine-tuning, but **you should experiment with this value**
      #lr: !!float 2e-6 # USE LOWER LR for LION
      triton: false # ONLY RELEVANT FOR LION
      weight_decay: 0.0
      beta1: 0.9
      beta2: 0.96
    clip_grad_eps: 1.0

    injectors:  # TODO: replace this entire sequence with the GptVoiceLatentInjector
      paired_fwd_text:
        type: generator
        generator: gpt
        in: [input_ids, output_ids, input_type, mel, mel_lengths]
        out: [loss_ce, acc, ctc_loss]
    losses:
      ce:
        type: direct
        weight: 1
        key: loss_ce
      ctc:
        type: direct
        weight: 0.1
        key: ctc_loss

path:
  # pretrain_model_gpt: '../experiments/autoregressive.pth' # CHANGEME: copy this from tortoise cache
  strict_load: true
  resume_state: !join [*exp_dir, /training_state/]   # <-- Set this to resume from a previous training state.

  # pretrain_model_gpt: /home/wumenglin/repo-dev/DL-Art-School-dev/experiments/minmo_asr_librispeech+gigaspeech_conformer_chunk240ms-25hz-look_forward-590k-11-layers-folded_ffn-no-wd-noise-12.5hz-7b/models/20000_gpt.pth  # CHANGEME: copy this from tortoise cache
  # strict_load: true
  # resume_state: !join [*exp_dir, /training_state/]   # <-- Set this to resume from a previous training state.
  # optimizer_reset: True
  # scheduler_reset: True

# afaik all units here are measured in **steps** (i.e. one batch of batch_size is 1 unit)
train: # CHANGEME: ALL OF THESE PARAMETERS SHOULD BE EXPERIMENTED WITH
  niter: 40000
  warmup: 2500
  warmup_iter: 2500
  mega_batch_factor: 1    # <-- Gradient accumulation factor. If you are running OOM, increase this to [2,4,8].
  val_freq: 5000 # TODO set this to epoch size * something

  default_lr_scheme: MultiStepLR
  gen_lr_steps: [5000, 10000, 18000, 30000]
  lr_gamma: 0.5
  ema_enabled: false
  manual_seed: *manual_seed # add this if you want reproducibility

eval:
  pure: true # see train.py

logger:
  print_freq: 10 # TODO: set this to epoch size
  save_checkpoint_freq: 5000 # CHANGEME: especially you should increase this it's really slow
  visuals: [gen, mel] #TODO: figure this out
  visual_debug_rate: 10000000
  is_mel_spectrogram: true
  disable_state_saving: false # CHANGEME if you plan to halt training inbetween

upgrades:
  # Variable: num_of_checkpoints_to_save
  # Description: Define how many checkpoints should be saved on disk (1 checkpoint = pth+ =~ 6.8 GB)
  # Type: integer
  # Value: should be the same value as for num_of_states_to_save
  # smaller than 1 - turn off this option; there is no max value. For Colab use 1 or 2.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved checkpoint + last saved state (about 6.8 GB).
  # 2 == Leave last 2 saved checkpoints + last saved states (about 2 *~ 6.8 GB =~ 13.6 GB).
  number_of_checkpoints_to_save: 5
  # Variable: num_of_states_to_save
  # Description: Define how many states should be saved on disk (1 state =~ 3.4 GB)
  # if disable_state_saving is set as true this option will be inactive
  # Type: integer
  # Value: should be the same value as for num_of_checkpoints_to_save
  # smaller than 1 - turn off this option; there is no max value.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved state (about 3.4 GB).
  # 2 == Leave last 2 saved states (about 2 *~ 3.4 GB =~ 6.8 GB).
  number_of_states_to_save: 5
