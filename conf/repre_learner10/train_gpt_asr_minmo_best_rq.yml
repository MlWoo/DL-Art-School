name: &name minmo_asr_librispeech_conformer_chunk160ms-v2
model: extensibletrainer
scale: 1
gpu_ids: [0] # <-- unless you have multiple gpus, use this
start_step: 0 # -1 causes 0.pth to be saved!
checkpointing_enabled: False  # <-- Gradient checkpointing. Enable for huge GPU memory savings. Disable for distributed training.
fp16: &fp16 true # TODO: why does enabling this with 8bit slow down perf??
half_type: &half_type bf16
use_8bit: false
wandb: false  # <-- enable to log to wandb. tensorboard logging is always enabled.
use_tb_logger: true
cuda_benchmarking_enabled: true # no convs in the model
raise_oom: true
ddp_find_unused_parameters: true
anomaly_detection: false

exp_dir: &exp_dir
  !join [../experiments/, *name]

qwen2_pretrained_path: &qwen2_pretrained_path
  "qwen/Qwen2.5-1.5B-Instruct"


text_type: &text_type 0
speech_type: &speech_type 1
asr_text_type: &asr_text_type 2
feature_size: &feature_size 128


manual_seed: &manual_seed 42
mel_bins: &mel_bins 128

audio_process: &audio_process
  sampling_rate: &sr 16000
  mel: &mel_cfg
    mel_fmin: 0
    mel_fmax: 8000
    sampling_rate: *sr
    n_mel_channels: *mel_bins
    filter_length: 1024
    hop_length: 160
    win_length: 640
    true_normalization: false
    mel_norm_file: /home/wumenglin/repo-dev/dl-art-school/codes/torch_mels.pt


dataloaders:
  train:
    # data loader background
    buffer_background: true
    buffer_background_size: 2
    process_background: true
    process_device: 0
    n_workers: 6
    pin_memory: true

    dataset:
      phase: train
      seed: *manual_seed
      name: LibriSpeech
      mode: HuggingfaceMinmoASRLhotseDataset
      store_type: json #json
      data_file:
        - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-clean-100.jsonl
        - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-clean-360.jsonl
        - /home/wumenglin/data/librispeech-asr/librispeech_cuts_train-other-500.jsonl
      read_keys:
        - text
        - text_token
        - source
      cache_dir: /home/wumenglin/.cache/
      min_duration: 0.3
      max_duration: 30
      sample_duration: 40
      file_path_key: id
      cache_text: true

      shrink: false
      sorted: false
      carry_filename: false

      tokenizers:
        text: *qwen2_pretrained_path
      text_hint_id: 6562
      audio_eos_id: 6563
      text_inter_len: 5
      audio_inter_len: 15
      ignore_id: -100
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type

    sampler:
      batch_size: 4
      buffer_batch_group: 2
      bucket_batch_volume: 32
      similar_type: text_token_length
      last_samples: 'drop'
      shuffle: true
      limited_length_type: text_token_length
      length_range: [0, -1]
      copies: 1
      persistent_workers: True
      pad_to_samples: 200
      batch_mode: dynamical
      num_buckets: 16
      ## dynamical data sampler
      max_tokens: 24_000_000_000 # 15 * 768 ^ 1.2 # 34802, 43503 120000/a100
      acc_coeffs: [[2.0, 480], [1.0, 1400000]]
      max_samples: 256
      partition_record_path: /home/wumenglin/.cache/LibriSpeech/dynamic_partition_7

      ## bucketed data sampler
      bucket_max_batch_tokens: 16384
      bucket_min_samples: 1
      bucket_max_samples: 180

    collator:
      mode: MelsInferCollator
      apply_half: *fp16
      half_type: *half_type

      spec_fn: 'canonical'
      audio_process: *audio_process
      pad_mode: trim
      data_cfg:
        input_ids:
          padding_val: 151643
        output_ids:
          padding_val: -100
        input_type:
          padding_val: 3
        input_lengths:
          padding_val: -1
        audio:
          padding_val: 0
        audio_lengths:
          padding_val: -1

  val:
    buffer_background: false
    buffer_background_size: 2
    process_background: true
    process_device: 0
    n_workers: 6
    pin_memory: true

    dataset:
      phase: train
      name: LibriSpeech
      seed: *manual_seed
      mode: HuggingfaceMinmoASRLhotseDataset
      store_type: json #json
      data_file:
      - /home/wumenglin/data/librispeech-asr/librispeech_cuts_dev-clean.jsonl
      - /home/wumenglin/data/librispeech-asr/librispeech_cuts_dev-other.jsonl
      cache_dir: /home/wumenglin/.cache/
      partition_record_path: /home/wumenglin/.cache/LibriSpeech/dynamic_partition_6
      min_duration: 0.3
      max_duration: 30
      sample_duration: 40
      file_path_key: id
      cache_text: true
      read_keys:
        - text
        - text_token
        - source
      shrink: false
      sorted: false
      carry_filename: false

      tokenizers:
        text: *qwen2_pretrained_path
      text_hint_id: 6562
      audio_eos_id: 6563
      text_inter_len: 5
      audio_inter_len: 15
      ignore_id: -100
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type

    sampler:
      batch_size: 24
      buffer_batch_group: 2
      bucket_batch_volume: 32
      similar_type: text_token_length
      last_samples: 'drop'
      shuffle: true
      limited_length_type: text_token_length
      length_range: [0, -1]
      copies: 1
      persistent_workers: True
      pad_to_samples: 200
      batch_mode: fixed
      num_buckets: 16
      ## dynamical data sampler
      max_tokens: 8000000000 # 15 * 768 ^ 1.2 # 34802, 43503 120000/a100
      acc_coeffs: [[2.0, 480], [1.0, 1400000]]
      max_samples: 256

      ## bucketed data sampler
      bucket_max_batch_tokens: 16384
      bucket_min_samples: 1
      bucket_max_samples: 180

    collator:
      mode: MelsInferCollator
      apply_half: *fp16
      half_type: *half_type

      spec_fn: 'canonical'
      audio_process: *audio_process
      pad_mode: trim
      data_cfg:
        input_ids:
          padding_val: 151643
        output_ids:
          padding_val: -100
        input_type:
          padding_val: 3
        input_lengths:
          padding_val: -1
        audio:
          padding_val: 0
        audio_lengths:
          padding_val: -1

num_model_dim: &num_model_dim 1024
networks:
  gpt:
    type: generator
    which_model_G: qwen2_asr # none of the unified_voice*.py files actually match the tortoise inference code... 4 and 3 have "alignment_head" (wtf is that?), 2 lacks the types=1 parameter.
    kwargs:
      pretrain_path: *qwen2_pretrained_path
      text_type: *text_type
      speech_type: *speech_type
      asr_text_type: *asr_text_type
      feature_size: *feature_size
      projector_type: folded
      stream: true
      speech_encoder_type: conformer
      encoder:
        class_name: model.audio.module.bestrq_flex.BestRqConformerEncoder
        input_dim: *mel_bins
        input_channels: 1
        num_attention_heads: 8
        hidden_size: *num_model_dim
        ffn_dim: 4096
        num_hidden_layers: 20
        conv_depthwise_kernel_size: 4
        feat_proj_dropout: 0.
        activation_dropout: 0.
        hidden_dropout: 0.
        max_source_positions: 3000
        no_scale_embedding: false
        hidden_act: "swish"
        conformer_conv_dropout: 0.
        position_embeddings_type: relative
        attention_dropout: 0.
        rotary_embedding_base: 10000
        layerdrop: 0.
        final_dropout: 0.
        num_preconformer_layers: 3
        num_preconformer_heads: 4
        preconformer_hidden_size: 384
        preconformer_ffn_dim: 1536
        preconformer_input_feature_projection: false
        causal: true
        sub_layers: 1
        conv_hidden_size: [8,]
        compile: false
        window_size: 64
        stream_chunk_size: 8 #160ms

      encoder_pretrained: /home/wumenglin/repo-dev/DL-Art-School-dev/experiments/streamconformer-bestrq-large_lr_h100x8_gelu-causal-sub1/models/310000_generator.pth
      encoder_freeze: true
      encoder_layer_idx: 16
      enable_output_ln: false

steps:
  gpt_train:
    training: gpt
    loss_log_buffer: 500 # no idea what this does
    check_abnorm_grads: true
    # Generally follows the recipe from the DALLE paper.
    optimizer: adamw # this should be adamw_zero if you're using distributed training
    #optimizer: lion
    optimizer_params:
      lr: !!float 1e-4 # CHANGEME: this was originally 1e-4; I reduced it to 1e-5 because it's fine-tuning, but **you should experiment with this value**
      #lr: !!float 2e-6 # USE LOWER LR for LION
      triton: false # ONLY RELEVANT FOR LION
      weight_decay: !!float 1e-4
      beta1: 0.9
      beta2: 0.96
    clip_grad_eps: 1.0

    injectors:  # TODO: replace this entire sequence with the GptVoiceLatentInjector
      paired_fwd_text:
        type: generator
        generator: gpt
        in: [input_ids, output_ids, input_type, mel, mel_lengths]
        out: [loss_ce, acc, ctc_loss]
    losses:
      ce:
        type: direct
        weight: 1
        key: loss_ce
      ctc:
        type: direct
        weight: 0.1
        key: ctc_loss

path:
  #pretrain_model_gpt: '../experiments/autoregressive.pth' # CHANGEME: copy this from tortoise cache
  strict_load: true
  resume_state: !join [*exp_dir, /training_state/]   # <-- Set this to resume from a previous training state.

# afaik all units here are measured in **steps** (i.e. one batch of batch_size is 1 unit)
train: # CHANGEME: ALL OF THESE PARAMETERS SHOULD BE EXPERIMENTED WITH
  niter: 40000
  warmup: 2500
  warmup_iter: 2500
  mega_batch_factor: 1    # <-- Gradient accumulation factor. If you are running OOM, increase this to [2,4,8].
  val_freq: 5000 # TODO set this to epoch size * something

  default_lr_scheme: MultiStepLR
  gen_lr_steps: [5000, 10000, 18000, 30000]
  lr_gamma: 0.5
  ema_enabled: false
  manual_seed: *manual_seed # add this if you want reproducibility

eval:
  pure: true # see train.py

logger:
  print_freq: 10 # TODO: set this to epoch size
  save_checkpoint_freq: 5000 # CHANGEME: especially you should increase this it's really slow
  visuals: [gen, mel] #TODO: figure this out
  visual_debug_rate: 10000000
  is_mel_spectrogram: true
  disable_state_saving: false # CHANGEME if you plan to halt training inbetween

upgrades:
  # Variable: num_of_checkpoints_to_save
  # Description: Define how many checkpoints should be saved on disk (1 checkpoint = pth+ =~ 6.8 GB)
  # Type: integer
  # Value: should be the same value as for num_of_states_to_save
  # smaller than 1 - turn off this option; there is no max value. For Colab use 1 or 2.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved checkpoint + last saved state (about 6.8 GB).
  # 2 == Leave last 2 saved checkpoints + last saved states (about 2 *~ 6.8 GB =~ 13.6 GB).
  number_of_checkpoints_to_save: 5
  # Variable: num_of_states_to_save
  # Description: Define how many states should be saved on disk (1 state =~ 3.4 GB)
  # if disable_state_saving is set as true this option will be inactive
  # Type: integer
  # Value: should be the same value as for num_of_checkpoints_to_save
  # smaller than 1 - turn off this option; there is no max value.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved state (about 3.4 GB).
  # 2 == Leave last 2 saved states (about 2 *~ 3.4 GB =~ 6.8 GB).
  number_of_states_to_save: 5
